{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import random\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from skimage import color\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.utils import make_grid\n",
    "import nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pickling(file,path):\n",
    "    pickle.dump(file,open(path,'wb'))\n",
    "def unpickling(path):\n",
    "    file_return=pickle.load(open(path,'rb'))\n",
    "    return file_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_segments = [  0,   2,   3,   4,   5,   7,   8,  10,  11,  12,  13,  14,  15,\n",
    "        16,  17,  18,  24,  26,  28,  30,  31,  41,  42,  43,  44,  46,\n",
    "        47,  49,  50,  51,  52,  53,  54,  58,  60,  62,  63,  72,  77,\n",
    "        80,  85, 251, 252, 253, 254, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_available = [11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coordinate = np.zeros((256,256))\n",
    "for i in range(256):\n",
    "    x_coordinate[i:] = [i]*256    \n",
    "y_coordinate = x_coordinate.copy()\n",
    "y_coordinate = y_coordinate.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainImages(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, train_data = False, flipping = True, rotation = True, translation = True):\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.flipping = flipping\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dir)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        image_path = \"/gpfs/data/\"+self.image_dir[idx]\n",
    "        aseg_path = \"/gpfs/data/\"+self.label_dir[idx]\n",
    "        \n",
    "        img_orig = nibabel.freesurfer.mghformat.MGHImage.from_filename(image_path)\n",
    "        image = img_orig.get_data().astype(np.float64)\n",
    "        aseg_img = nibabel.freesurfer.mghformat.MGHImage.from_filename(aseg_path)\n",
    "        aseg_img = aseg_img.get_data().astype(np.float64)\n",
    "\n",
    "        flip = random.random() > 0.5\n",
    "        angle = random.uniform(-5,5)\n",
    "        dx = np.round(random.uniform(-10,10))\n",
    "        dy = np.round(random.uniform(-10,10))\n",
    "        \n",
    "        im = Image.fromarray(image[0])\n",
    "        target = Image.fromarray(aseg_img[0])\n",
    "        if self.train_data:\n",
    "            if self.flipping and flip:\n",
    "                im = im.transpose(0)\n",
    "                target = target.transpose(0)\n",
    "            if self.rotation:\n",
    "                im = im.rotate(angle)\n",
    "                target = target.rotate(angle)\n",
    "            if self.translation:\n",
    "                im = im.transform((256,256),0, (1,0,dx,0,1,dy))\n",
    "                target = target.transform((256,256),0,(1,0,dx,0,1,dy))\n",
    "                \n",
    "#        im = torch.from_numpy(np.array(im, np.float64, copy=False).reshape((1,256,256)))/255\n",
    "        im = torch.from_numpy(np.array([np.array(im, np.float64, copy=False), x_coordinate, y_coordinate], np.float64, copy=False))\n",
    "\n",
    "\n",
    "        target = np.array(target, np.float64, copy=False)\n",
    "        target_label = np.zeros((2,256,256))\n",
    "        for i,a in enumerate(available_segments):\n",
    "            temp = (target==a).astype(int)\n",
    "            if a in rest_available:\n",
    "                target_label[rest_available.index(a),:,:] = temp\n",
    "            else:\n",
    "                target_label[1,:,:] = target_label[1,:,:] + temp\n",
    "        target_label[1,:,:] = (target_label[1,:,:]>=1).astype(int) \n",
    "#        print(target_label.shape)\n",
    "        target_label = torch.from_numpy(target_label)\n",
    "        sample = {'x':im,'y':target_label} \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = pd.read_csv(\"all_complete_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects = unpickling(\"train_subject_index\")\n",
    "val_subjects = unpickling(\"val_subject_index\")\n",
    "test_subjects = unpickling(\"test_subject_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_raw = list(file_names.iloc[train_subjects,2])\n",
    "full_train_seg = list(file_names.iloc[train_subjects,3])\n",
    "\n",
    "full_val_raw = list(file_names.iloc[val_subjects,2])\n",
    "full_val_seg = list(file_names.iloc[val_subjects,3])\n",
    "\n",
    "full_test_raw = list(file_names.iloc[test_subjects,2])\n",
    "full_test_seg = list(file_names.iloc[test_subjects,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4986"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "277*18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand1 = np.arange(len(full_train_raw))\n",
    "np.random.shuffle(rand1)\n",
    "rand1 = rand1[:4986]\n",
    "\n",
    "rand2 = np.arange(len(val_subjects))\n",
    "np.random.shuffle(rand2)\n",
    "rand2 = rand2[:1000]\n",
    "\n",
    "rand3 = np.arange(len(test_subjects))\n",
    "np.random.shuffle(rand3)\n",
    "rand3 = rand3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = {'train': BrainImages(np.array(full_train_raw)[rand1],np.array(full_train_seg)[rand1], train_data= True, flipping=False),\n",
    "                       'validate': BrainImages(np.array(full_val_raw)[rand2],np.array(full_val_seg)[rand2]),\n",
    "                       'test': BrainImages(np.array(full_test_raw)[rand3],np.array(full_test_seg)[rand3])\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=18,\n",
    "                        shuffle=True, num_workers=0) for x in ['train', 'validate','test']}\n",
    "data_sizes ={x: len(transformed_dataset[x]) for x in ['train', 'validate','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n"
     ]
    }
   ],
   "source": [
    "df_seg = pd.DataFrame(columns=np.arange(2))\n",
    "ll = 0\n",
    "batch = 8\n",
    "for data in dataloader[\"train\"]:\n",
    "    print(ll)\n",
    "    y = data['y'].numpy()\n",
    "    #print(y.shape)\n",
    "    for i in range(batch):\n",
    "        for j in range(2):\n",
    "            s = np.sum(y[i,j,:,:])\n",
    "            df_seg.loc[ll*batch+i,j] = s\n",
    "    ll = ll + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2216, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Downsample_block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(y, 2,stride = 2)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample_block(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Upsample_block, self).__init__()\n",
    "        self.transconv = nn.ConvTranspose2d(in_channels, out_channels, 4, padding = 1, stride = 2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self,x, y):\n",
    "        x = self.transconv(x)\n",
    "        x = torch.cat((x,y),dim = 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        self.down1 = Downsample_block(3,64)\n",
    "        self.down2 = Downsample_block(64,128)\n",
    "        self.down3 = Downsample_block(128,256)\n",
    "        self.down4 = Downsample_block(256,512)\n",
    "        self.conv1 = nn.Conv2d(512,1024, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(1024)\n",
    "        self.conv2 = nn.Conv2d(1024,1024,3, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(1024)\n",
    "        self.up4 = Upsample_block(1024,512)\n",
    "        self.up3 = Upsample_block(512,256)\n",
    "        self.up2 = Upsample_block(256,128)\n",
    "        self.up1 = Upsample_block(128,64)\n",
    "        self.outconv = nn.Conv2d(64,2, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x, y1 = self.down1(x)\n",
    "        x, y2 = self.down2(x)\n",
    "        x, y3 = self.down3(x)\n",
    "        x, y4 = self.down4(x)\n",
    "        x = F.dropout2d(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = F.dropout2d(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.up4(x, y4)\n",
    "        x = self.up3(x, y3)\n",
    "        x = self.up2(x, y2)\n",
    "        x = self.up1(x, y1)\n",
    "        x = self.outconv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_num = np.mean(np.array(df_seg),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_torch = Variable(torch.from_numpy(np.array(wts_num))).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_2(true1,scores1,mean, epsilon = 1e-4,p = 1.25):\n",
    "    N, C, sh1, sh2 = true1.size()\n",
    "    \n",
    "    true = []\n",
    "    scores = []\n",
    "    for c in range(N):\n",
    "        if not torch.sum(true1[c,1:,:,:])==0:\n",
    "            true.append(true1[c,:,:,:])\n",
    "            scores.append(scores1[c,:,:,:])\n",
    "    try:\n",
    "        true = torch.stack(true)\n",
    "        scores = torch.stack(scores)\n",
    "    except:\n",
    "        return -1\n",
    "    N, C, sh1, sh2 = true.size()\n",
    "    \n",
    "    \n",
    "    preds = F.softmax(scores)\n",
    "    true = true.view(N, C, -1)\n",
    "    preds = preds.view(N, C, -1)\n",
    "    wts = torch.sum(true, dim = 2) + epsilon\n",
    "    mean = 1/torch.pow(mean,p)\n",
    "    wts[:,:] = mean[None].repeat(N,1)\n",
    "    wts = wts/(torch.sum(wts,dim = 1)[:,None])\n",
    "    prod = torch.sum(true*preds,dim = 2)\n",
    "    sum_tnp = torch.sum(true + preds, dim = 2)\n",
    "    num = torch.sum(wts * prod, dim = 1)\n",
    "    denom = torch.sum(wts * sum_tnp, dim = 1) + epsilon\n",
    "    loss = 1 - 2*(num/denom)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(true1,scores1, epsilon = 1e-7):\n",
    "    N ,C, sh1, sh2 = true1.size()\n",
    "    true = []\n",
    "    scores = []\n",
    "    for c in range(N):\n",
    "        if not torch.sum(true1[c,1:,:,:])==0:\n",
    "            true.append(true1[c,:,:,:])\n",
    "            scores.append(scores1[c,:,:,:])\n",
    "    try:\n",
    "        true = torch.stack(true)\n",
    "        scores = torch.stack(scores)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    N, C, sh1, sh2 = true.size()\n",
    "    \n",
    "    true = true.view(N,C,-1)\n",
    "    preds = F.softmax(scores)\n",
    "    pred_class = (torch.max(preds, dim = 1)[1]).view(N,-1)\n",
    "    class_score = [0]*2\n",
    "    for i in range(2):\n",
    "        class_score[i] = (pred_class == i).data.type(torch.cuda.FloatTensor)\n",
    "    true = true.data.type(torch.cuda.FloatTensor)\n",
    "    def numerator(truth,pred, idx):\n",
    "        return(torch.sum(truth[:,idx,:] * pred,dim = 1)) + epsilon/2\n",
    "    def denominator(truth,pred,idx):\n",
    "        return(torch.sum(truth[:,idx,:]+pred,dim = 1)) + epsilon\n",
    "    dice_class_score = [0]*2\n",
    "    for i in range(2):\n",
    "        dice_class_score[i] = torch.mean(2*(numerator(true,class_score[i],i))/(denominator(true,class_score[i],i)))\n",
    "    return dice_class_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_model(model, optimizer,dataloader, data_sizes, batch_size, num_epochs = 100, verbose = False):\n",
    "    since = time.time()\n",
    "    best_loss = np.inf\n",
    "    loss_hist = {'train':[],'validate':[]}\n",
    "    dice_scores_of_all_class = [{'train':[],'validate':[]} for i in range(2)]    \n",
    "    for i in range(num_epochs):\n",
    "        for phase in ['train', 'validate']:\n",
    "            running_loss = 0\n",
    "            run_class_scores = [0]*2\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "    \n",
    "            for data in dataloader[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                x = data['x']\n",
    "                y = data['y']\n",
    "                x = Variable(x).type(torch.FloatTensor).cuda()\n",
    "                y = Variable(y).type(torch.FloatTensor).cuda()\n",
    "                \n",
    "                output = model(x)\n",
    "                \n",
    "#                 y_sel = []\n",
    "#                 output_sel = []\n",
    "#                 for sel in range(x.size()[0]):\n",
    "#                     if torch.sum(y[sel,0,:,:]) != 0:\n",
    "#                         y_sel.append(y[sel,:,:,:])\n",
    "#                         output_sel.append(y[sel,:,:,:])\n",
    "                        \n",
    "#                 try:\n",
    "#                     y_sel = torch.stack(y_sel)\n",
    "#                     output_sel = torch.stack(output_sel)\n",
    "#                 except:\n",
    "#                     continue\n",
    "                loss = dice_loss_2(y, output,wts_torch)\n",
    "                #loss = Variable(dice_loss_2(y_sel, output_sel,wts_torch),requires_grad=True)\n",
    "                if loss==-1:\n",
    "                    continue\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.data[0] * batch_size\n",
    "                dice_score_batch = dice_score(y,output)\n",
    "\n",
    "                for j in range(2):\n",
    "                    run_class_scores[j] = run_class_scores[j] + dice_score_batch[j] * batch_size\n",
    "            epoch_loss = running_loss/data_sizes[phase]\n",
    "            loss_hist[phase].append(epoch_loss.item()) \n",
    "            for j in range(2):\n",
    "                score = run_class_scores[j]/data_sizes[phase]\n",
    "                dice_scores_of_all_class[j][phase].append(score.to(torch.device(\"cpu\")).numpy())\n",
    "            if verbose or i%1 == 0:\n",
    "                print('Epoch: {}, Phase: {}, epoch loss: {:.4f}'.format(i,phase,epoch_loss))\n",
    "                print('-'*10)\n",
    "            \n",
    "        if phase == 'validate' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = model.state_dict() \n",
    "        #print(i)\n",
    "    print('-'*50)    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val dice loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, loss_hist, dice_scores_of_all_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet().cuda()\n",
    "model = nn.DataParallel(model)\n",
    "criterion = dice_loss_2\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr = 1e-4)\n",
    "#scheduler = lr_scheduler.StepLR(optimizer,step_size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Phase: train, epoch loss: 0.9988\n",
      "----------\n",
      "Epoch: 0, Phase: validate, epoch loss: 1.0069\n",
      "----------\n",
      "Epoch: 1, Phase: train, epoch loss: 0.9986\n",
      "----------\n",
      "Epoch: 1, Phase: validate, epoch loss: 1.0070\n",
      "----------\n",
      "Epoch: 2, Phase: train, epoch loss: 0.9981\n",
      "----------\n",
      "Epoch: 2, Phase: validate, epoch loss: 1.0064\n",
      "----------\n",
      "Epoch: 3, Phase: train, epoch loss: 0.9743\n",
      "----------\n",
      "Epoch: 3, Phase: validate, epoch loss: 0.8174\n",
      "----------\n",
      "Epoch: 4, Phase: train, epoch loss: 0.6697\n",
      "----------\n",
      "Epoch: 4, Phase: validate, epoch loss: 0.5747\n",
      "----------\n",
      "Epoch: 5, Phase: train, epoch loss: 0.5577\n",
      "----------\n",
      "Epoch: 5, Phase: validate, epoch loss: 0.5181\n",
      "----------\n",
      "Epoch: 6, Phase: train, epoch loss: 0.5172\n",
      "----------\n",
      "Epoch: 6, Phase: validate, epoch loss: 0.4942\n",
      "----------\n",
      "Epoch: 7, Phase: train, epoch loss: 0.5086\n",
      "----------\n",
      "Epoch: 7, Phase: validate, epoch loss: 0.4955\n",
      "----------\n",
      "Epoch: 8, Phase: train, epoch loss: 0.1984\n",
      "----------\n",
      "Epoch: 8, Phase: validate, epoch loss: 0.1233\n",
      "----------\n",
      "Epoch: 9, Phase: train, epoch loss: 0.1200\n",
      "----------\n",
      "Epoch: 9, Phase: validate, epoch loss: 0.1145\n",
      "----------\n",
      "Epoch: 10, Phase: train, epoch loss: 0.1168\n",
      "----------\n",
      "Epoch: 10, Phase: validate, epoch loss: 0.1143\n",
      "----------\n",
      "Epoch: 11, Phase: train, epoch loss: 0.1172\n",
      "----------\n",
      "Epoch: 11, Phase: validate, epoch loss: 0.1405\n",
      "----------\n",
      "Epoch: 12, Phase: train, epoch loss: 0.1196\n",
      "----------\n",
      "Epoch: 12, Phase: validate, epoch loss: 0.7438\n",
      "----------\n",
      "Epoch: 13, Phase: train, epoch loss: 0.1212\n",
      "----------\n",
      "Epoch: 13, Phase: validate, epoch loss: 0.9985\n",
      "----------\n",
      "Epoch: 14, Phase: train, epoch loss: 0.1201\n",
      "----------\n",
      "Epoch: 14, Phase: validate, epoch loss: 1.0001\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Training complete in 63m 46s\n",
      "Best val dice loss: 0.114334\n"
     ]
    }
   ],
   "source": [
    "model, loss_hist, dice_hist = train_model(model, optimizer,dataloader,data_sizes,18,\n",
    "                                                                num_epochs = 15, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Phase: train, epoch loss: 0.1187\n",
      "----------\n",
      "Epoch: 0, Phase: validate, epoch loss: 0.3239\n",
      "----------\n",
      "Epoch: 1, Phase: train, epoch loss: 0.1174\n",
      "----------\n",
      "Epoch: 1, Phase: validate, epoch loss: 0.5471\n",
      "----------\n",
      "Epoch: 2, Phase: train, epoch loss: 0.1160\n",
      "----------\n",
      "Epoch: 2, Phase: validate, epoch loss: 0.8951\n",
      "----------\n",
      "Epoch: 3, Phase: train, epoch loss: 0.1147\n",
      "----------\n",
      "Epoch: 3, Phase: validate, epoch loss: 0.9650\n",
      "----------\n",
      "Epoch: 4, Phase: train, epoch loss: 0.1141\n",
      "----------\n",
      "Epoch: 4, Phase: validate, epoch loss: 1.0023\n",
      "----------\n",
      "Epoch: 5, Phase: train, epoch loss: 0.1154\n",
      "----------\n",
      "Epoch: 5, Phase: validate, epoch loss: 0.3899\n",
      "----------\n",
      "Epoch: 6, Phase: train, epoch loss: 0.1137\n",
      "----------\n",
      "Epoch: 6, Phase: validate, epoch loss: 0.5935\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr = 5e-5)\n",
    "\n",
    "model, loss_hist, dice_hist = train_model(model, optimizer,dataloader,data_sizes,18,\n",
    "                                                                num_epochs = 15, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
